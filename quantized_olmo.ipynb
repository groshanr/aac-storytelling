{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Modified from: https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_(7B)-Text_Completion.ipynb#scrollTo=XsysvFz1KqN0"
      ],
      "metadata": {
        "id": "eILBFBdsIuUA"
      },
      "id": "eILBFBdsIuUA",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.56.2\n",
        "!pip install --no-deps trl==0.22.2"
      ],
      "metadata": {
        "id": "ml4DhPsjIslG"
      },
      "id": "ml4DhPsjIslG",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%env UNSLOTH_RETURN_LOGITS=1 # Run this to disable CCE since it is not supported for CPT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmnj-5ZpI35j",
        "outputId": "4fcefe24-7cbc-4933-94de-359ba0c6342f"
      },
      "id": "hmnj-5ZpI35j",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: UNSLOTH_RETURN_LOGITS=1 # Run this to disable CCE since it is not supported for CPT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "93d6b3ac",
      "metadata": {
        "id": "93d6b3ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91bbe2f5-d2d4-41fd-c155-7e23d24ff129"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel, FastModel, UnslothTrainer, UnslothTrainingArguments\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name = \"allenai/OLMo-2-1124-7B\",\n",
        "    max_seq_length = 2048,\n",
        "    load_in_4bit = True\n",
        ")"
      ],
      "metadata": {
        "id": "W7yvF708yILP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575,
          "referenced_widgets": [
            "d560980d6600494e858eeebcc00fdd7c",
            "7480950b09844ed18fc8a827d113f653",
            "c41eefeed38641a9ba0aad598b1cd4b3",
            "578b66e7cc9544e2a4023ca96bf57c8e",
            "2a1824db9c014ad883a32aee0eee7062",
            "e7dd58081ae648499e7e8753e87a2213",
            "a85f55f45fc44ac8a8fdb746dc47cefb",
            "07ead9e563a44d8ebf93af5142107926",
            "8d045c316dcd4e8498dfff2b4be00200",
            "e5404290772345f59712f9fdf54f6b77",
            "f240728c9b0648769f83a2c67251032c",
            "c66468878c2e4afaae84fb1ccffcfa0e",
            "cd95784eff044653a0b651405e6413c0",
            "2ff806cfda874cdd9e2caeccb4e0c195",
            "0ee767294b65412bb8a78a95dfbdfb7f",
            "6d1d349a4d764bd6994306b886935234",
            "efc2c93d5efc492d97c5d7345311f4f7",
            "a44015b1067b4c309809315297510031",
            "9a1fb836ab5b41f2998b230595addbf5",
            "9b3978af67b04925a47f9b44a89b0cbb",
            "b99f7d9d8b9248dd9fbb1063ff0aa254",
            "1e737e417a9a48bdbb645bdf5711b468",
            "11e900c7868e40f79f8d9a43c6d1fffa",
            "c15c563183664bbc82d284cbe9ccb0e5",
            "96c6852efb744c5a86cc746e8f4d7ff4",
            "4b01a67f55b94003b734ac9138111930",
            "e6ef748ddc444f7481b512599cb9d86e",
            "680bce69c06e4cb5b66b035c254b2ce0",
            "ae27194326d24165a0c8dfa77c052e68",
            "2d29d7a77cb34b9a93b31f3f91470350",
            "6aea6f610d9541308fbf94cf08eb1848",
            "4f54b9f2d5dd4dae868f5a13e28194ae",
            "77a6f837b87f475da619ac9fa69446eb",
            "04f9a3b7fc5b4827a6ccd08ad3a4812a",
            "f4cb9b27703a438d8e892b1032268c50",
            "591ac1dabba6480284baf882b464cc26",
            "8bb61eb7b166472aaf9148882c28cbf6",
            "6891b5a7417d4126bb4ffaa48b1f1c7e",
            "7a97e0781517432fa673dcc58eee54b0",
            "f4e42a9c455646ada0e7e38b66c4d267",
            "f6b7a92c9ca34387ae24d523cd7a4caf",
            "b7ef86041e5540cb9f53349caee2e25b",
            "023ea3eaee884657b3e1edfc98a8a9be",
            "2f3b89fdef004f6ab369f0f50cffd25c",
            "638350565eb647b7968a9f83c1fcc9ce",
            "f0afd9281026433096c742e488c9626c",
            "1508b14bc21343e190e37d082d58b7af",
            "8378c661ad9f4a92b8af478924938736",
            "9bd44d1b19ef4a1fb8da7d8a1a5e76cc",
            "d68bf662dabe4350869fb19950cd2292",
            "5b0b836c1b3c4d1497cea80ed9ae091b",
            "fadd1c74b6ff46bfa456639b7e779d41",
            "8fe16d584da44e789b731ea70ed47a96",
            "340d3363e6c9487490c62aa5204996d2",
            "b2dfa91d7fbc4612ab429999d41d8044",
            "8c3ca10b82ca4f58b82784ef84468be8",
            "3879114955304e558e41fc74c533ea58",
            "2fa9c6ad8d5d4b2299ef12df23847250",
            "14bc53ceffcb48bd98024100aebf9af3",
            "a45a7061dfdd4be38f4d8ac943279e67",
            "1ce6ed58c46540c3aa21c9f4726111e8",
            "d5067902049d45c58cf82453212446d9",
            "7f34d11bcb544aeea6d939ed55d92317",
            "dc249ab52ebb4dc3877ed5777c940b04",
            "8c3d7a0b857844ee85902a1a74e79106",
            "1e68ebe794cf4e6f922ea1afba5bca5a",
            "3cb6afda44704134ab105c3302f93002",
            "58e4ca6bdcfc47918a03cf2b4a0f2c7b",
            "ab2b24266f0c492a980148114a7b6a94",
            "415fc9aafecc47bfbc5db77a09726682",
            "49f6bf0f18e3435fa1cc5fc364974fcc",
            "2d350e0931fa428ebc37267b27a22c0c",
            "9b37a252b2b34ce49c0123fd20127367",
            "cbb7dfa4b3c44cc68ae975724699248d",
            "9c5f81ccabbd4a798cbcf2e6fb5bf441",
            "1bf758895e2447408fe76cfbe6f480d4",
            "2e76782af87a4290b230f9c364268cdc",
            "bb5d9aa77e8448d684ac672b4689a898",
            "a4e1202c41e446f493d8203e775b89e0",
            "71bc72f6515e49bdac8e40ff3f2b39ec",
            "456f90bafa5c4fedbbee240dea6de67f",
            "bc74e27a3c9f4dd3a00ef36466d0d363",
            "709ca377cf714fc09d215bfcfe85a3de",
            "eb8de3ceac5c40bdac8e83dca8144e4b",
            "f0c8f886bc684a0f9b870c0e74f7fc3e",
            "30f364c7f90741a6b87a8181dd57974f",
            "7eb43b5872c34b76bdc46d4ae57a4633",
            "09ba40ab0b004758bb944a9e061789d7",
            "3d733f583ddf454bb73b10651752a508",
            "5316180f1b3243e5a66367b694a70fb7",
            "b4aca2970bc240ffb72ec9a2e6c86801",
            "df17c304c96b48dc8c87a480b5aa1a73",
            "1a8293e38689427c9181da29679172bb",
            "53d2d05738034aa5a6ce444deb074089",
            "49db6897a9b94078bedf0636ab10d86f",
            "cefed50dbc314a0b8457078861f85870",
            "4ba7a85a95ba439b8d622d12beb214b5",
            "845344f0b92e46c59d3d6f82456ec7a6",
            "30940474e2d446778519e83a4321964e",
            "8ac478c88ed1414cb24545ef61736170",
            "5b00f590fbb74fa1b50fe326f9f5c449",
            "bf29aeec332648c1a23b34c4b20df6de",
            "d05906410a7644f0ba024cc517da1bfa",
            "aeeb1d8259f74948a328e8db774e674c",
            "3d742c5735c84fa89a722ce981ce029b",
            "40f133f5e48d43779bd73fec35a44294",
            "10fedae4c19d436c95aabfe31c4dcd68",
            "026008e947314d2cbb3e9d269accb2b8",
            "1f3df5c3c5f449248ab894c8f466643e",
            "81b8038496e6479ea7e904b4e5c069b7",
            "b987e7f2368e479b85e28fc201a691d9",
            "8772844e42d6487b8d9105b7b8a9219e",
            "5abec56e1745433bbbe53b5e89069711",
            "6fecf2d771a24b6c82d184001237fdae",
            "5c1508a3f17848ca84905d6d078fe8d8",
            "bb33c34ef0774def9a44981bb613b4ea",
            "d93e6f9d37894d28890fbfcb44b48f21",
            "4cd94bef83964c92b08278d02df6b20e",
            "121e3c9db3724508858292d19dc77f29",
            "9ef2ce191d144b68ba827d88ceed9a2f",
            "f5a7640e72384febbf4d7cd5e93ece0b",
            "6b989c7ebca94c3e864514bcd4630a47",
            "e2da59b694e047ab913ece2c041ed994",
            "db1b0a584e0342398ea4f39b4472ab32",
            "526f7c27e1164698a28165f2231c725c",
            "2baecaa7656b4564adc09e59148e4054",
            "1f685bfd23454dde8bdce366df08d9e5",
            "5af46d04332c4c20b1e0c721c39139e9",
            "5a43389edce04ad6ac54313380d53dd0",
            "47afba14474d4bd4a11b80920b9571cb",
            "eb2b60e25a144fbe9cd7682ede50fdfa",
            "876ef3323e844e9c8b349c44f66e3f7b",
            "7942e6f3052a4023bc84f48f4d028218",
            "6b362f7c83384d8ea1e984535fcfaabf",
            "6eae6d72daa047738f8dd30c88585eb8",
            "f82f58cc33ae44d387eef8d382c66409",
            "69123d90cf8f4af3b2ea68bc431b1410",
            "57234047462d49fe81ad2d8af97d4db1",
            "0d53fa3a4e54492690e301f05d5a3607",
            "2e19b2f612b949268579d62157fde5ff",
            "2f341e71ee8948149132b1baed5229f7",
            "f03acecdc4cd4f96b68c900c5eaaaea4",
            "ddf00b2455a54297857f2ac46097e836",
            "9cef04f0899e4c09b5a72ffe4c5bfe48",
            "f9c28586db2f434b99d23e0199f2cb3d",
            "19c2bd40a657418cad48904a04d325af",
            "6ab328980a1f4812b2a27c70faba8ad0",
            "5af85ae1227b45a1a8c87255c192adc1",
            "aaeb65bef4f04f508d6ccda8bee741bf",
            "b8ab91836c2d46b8a16d19d666c7ecca",
            "37c0b5718835462b91458061bf3ef876",
            "170e03c9a2e5494a9e17b6fc9c5c6eca",
            "c5fcb33bc29143659799d09f84dc38db",
            "56821250d7d049d2b9c328d0e45528b0"
          ]
        },
        "outputId": "bd134229-6de5-4cc7-f604-4bace07f6b95"
      },
      "id": "W7yvF708yILP",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.12.1: Fast Olmo2 patching. Transformers: 4.56.2.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.318 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 8.0. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post1. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d560980d6600494e858eeebcc00fdd7c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00006.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c66468878c2e4afaae84fb1ccffcfa0e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00006.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "11e900c7868e40f79f8d9a43c6d1fffa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00003-of-00006.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "04f9a3b7fc5b4827a6ccd08ad3a4812a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00004-of-00006.safetensors:   0%|          | 0.00/4.86G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "638350565eb647b7968a9f83c1fcc9ce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00005-of-00006.safetensors:   0%|          | 0.00/4.86G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8c3ca10b82ca4f58b82784ef84468be8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00006-of-00006.safetensors:   0%|          | 0.00/4.61G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3cb6afda44704134ab105c3302f93002"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bb5d9aa77e8448d684ac672b4689a898"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/126 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3d733f583ddf454bb73b10651752a508"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8ac478c88ed1414cb24545ef61736170"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b987e7f2368e479b85e28fc201a691d9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6b989c7ebca94c3e864514bcd4630a47"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7942e6f3052a4023bc84f48f4d028218"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9cef04f0899e4c09b5a72ffe4c5bfe48"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0cb0807d",
      "metadata": {
        "id": "0cb0807d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3354f25b-9951-4531-e7c1-af984d1220f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Making `model.base_model.model.model` require gradients\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'],\n",
        "    lora_alpha = 32,\n",
        "    qat_scheme = \"int4\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"roneneldan/TinyStories\", split = \"train[:2500]\")\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "def formatting_prompts_func(examples):\n",
        "    return { \"text\" : [example + EOS_TOKEN for example in examples[\"text\"]] }\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305,
          "referenced_widgets": [
            "f724c1a2b5894a7baa670158feadcfb8",
            "2a996cb1ce594b0697b0d55de4aedc32",
            "158979be6d4740f48f04ef32ee4c26b0",
            "bbae64001f794109b7323cd8d5c44241",
            "612f7b4b163549cb84d5efea50c8a6a9",
            "ad9725a703d24b089a8602d6aa0c9c8f",
            "5eaf63d7e6e14dbeadca3cf1d62b26f2",
            "101b403353fa4049b7472e2dee14c144",
            "2c8e941e33504a97827a37893caa97ef",
            "65cc016470f14291a0b5168e372ec4eb",
            "07729c2ced94431a9bccda3d0ca81779",
            "450d6a5e75804eacafc7b8a669b66c53",
            "aaa7b66e0f194c8280a8202ffb1710c8",
            "d3daf775e26347a29210d993aa138aec",
            "872b5e7dacc04ff4886573e1987fe2d8",
            "bdbad8dbe30446719138f9f7fed28837",
            "4499f93be6ca4051b21893f49f1eddaa",
            "8acade83867c43588a900177b20acb9c",
            "14de1f3967b84aedb2eab9f8a18e4bb3",
            "35fc7325815a478490ab32b7c7ac9ff5",
            "4b510a3db44148d4a96879e5a662f8dc",
            "3f10e1f0173e4725bcf64253370c1566",
            "e6dbe20cf0f8477c86a70c9b22e35f93",
            "0b185def81f741cf8e586ede8ed1178d",
            "dd2e2eda66c94041964a6f10a407243c",
            "ecca1b95a9a14f0f92d4d5a3605e4781",
            "5ece07bbb72246c29385cdb4b5605193",
            "c6e4aba5eb1c48b6b0b089465a311a76",
            "812f42e190df43a6b72dfa83e55f49fb",
            "fd6c0c645ce44627864baf0ca831e2cd",
            "7755492ec3974e489dc1f729f0929f70",
            "fbf4b289c04c40b5b5ffe639d75d66e3",
            "0e664906de1d4909a6f5e4cb614e8d3f",
            "1ec199e8a8c147319e6a82a999f29f77",
            "77743535ec5d499da18a0c8cb33e1983",
            "04959152528f460a9757daba29ce1c06",
            "892a2fe82c614cea923536fa96eab8d4",
            "aabab5fc3559461bb63292dbbc67ee14",
            "3f95eef6c3604b6ca7b193367d5e735a",
            "8ec7212f9994444e9cbcb33209ddaf2e",
            "aee53a223b8b48459f23aa59b99f3da5",
            "2baef6c76e38429e9787302ece128e44",
            "dd3a09df853d43a78594e93e7ae4625a",
            "a43d33153fc84af1b325e5c638fde3e5",
            "2c25e508c02e40cfa5da51878833448a",
            "c61d5c75a6ef483bb9424809aefa4b34",
            "54f91d82901f4809bfad70df18e3fa55",
            "aaef2d2fd3294775a98e0f37dc0e39de",
            "a99d4c0f61b446d0873e02d3d436a901",
            "d5f55f58d2214ac2ae233bd56295b7de",
            "79ef3e88441a434380da2f9fefccfc8b",
            "876f2b7f9cfb4bd486cad0525b31daab",
            "0a80561f654440b8a59e2ce60bf6b34f",
            "d89edb08691a4f5290fa92912a91c6cd",
            "5c76a00e6d7847ae84a4fc0418e5a753",
            "13e31e07f6c143849ea1e014eae03d9a",
            "2e560b36547043169db9461b379599b2",
            "84d1143a9b134178a8738bdd942caa7b",
            "b459dfe48e4f4096a1dd6d68f5cfdca4",
            "d6cbdecb21b14ef093dadd35f2c26ba2",
            "8d8cb1c9ec5a4e7bb5d294d0a73a5857",
            "c3d617655bf64c949dc0170cbb7baa75",
            "8a8f91ff7ae04949972036de5e5a54fc",
            "3ecfec6e39544fef905177fde0f12787",
            "fe552a0f180c4c9381f05f1ef50ad9a8",
            "f1698c0bc157427f923b4348da61b9b3",
            "49e22e9f611e4af79b5c38dcbd5a8106",
            "54fe5ee677d949bab662981fd5a6f08b",
            "5f0abc3eaee34e39864ad110ac4e9a86",
            "d0f6a5196e194a3fb69fbdcc3ccc3a91",
            "9d6a268dc11a49a58b640b55bf1aaf48",
            "f6a2daf1d82c4e1a85576a9dae2f7d14",
            "d8d1693db9614e7baa58fac537335ee3",
            "729eb14a8f96455982bd8111ca9373e5",
            "fa8b447e7b5643cbaac44bc7392e6638",
            "6325ec26a970420cb31ed4409ac08120",
            "30dee7eb2a0143039cb3ff8fb19d5024",
            "bb775d860c9e4f3cb259f6d61319b5c9",
            "e7db42c1fdf74037a6f01f30420a146a",
            "210ce6099a5a440b9eacc2e7f2d6f40c",
            "04526029d63a41eab000503390f669e7",
            "b2d69caec84b4c7aa7b32b48545da17d",
            "bb7e56ac56eb431aa6d08930743ab360",
            "482c76a80444489ea061c963f9b5b65c",
            "7c3adbb806f64d41a45980b3c7d09f21",
            "52abb9ea00444f8998557ed562f60df3",
            "d7b509d8809a4862aaa29ee40cca5405",
            "d67046445647422388fe266206355813",
            "e87efce6af02435c9b82bf95143aacb2",
            "e16f98b878aa45a6a62c51f72b814ede",
            "285fc966be7447ca9dd4332d56eaf935",
            "9e7e3e644a1a4db8a330276216ce708d",
            "9ffb62538d9d429698d234b2d2efff34",
            "ee95a3e33b424c289196c32266f9c930",
            "c457d3030d554f16b88728f683a9e757",
            "bbc2e0974eb242d8af5360929f10b4ad",
            "de6ff72919df4eb69a26359333b24e9f",
            "27d3f127a5c34be4adf7e4e6bda1d061",
            "89056af403124265a450ef7873725a54"
          ]
        },
        "id": "tVTNTrWcISzD",
        "outputId": "87a07e8c-9a46-4b13-c6ad-e134aae3d057"
      },
      "id": "tVTNTrWcISzD",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f724c1a2b5894a7baa670158feadcfb8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "data/train-00000-of-00004-2d5a1467fff108(â€¦):   0%|          | 0.00/249M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "450d6a5e75804eacafc7b8a669b66c53"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "data/train-00001-of-00004-5852b56a2bd28f(â€¦):   0%|          | 0.00/248M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e6dbe20cf0f8477c86a70c9b22e35f93"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "data/train-00002-of-00004-a26307300439e9(â€¦):   0%|          | 0.00/246M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1ec199e8a8c147319e6a82a999f29f77"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "data/train-00003-of-00004-d243063613e5a0(â€¦):   0%|          | 0.00/248M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2c25e508c02e40cfa5da51878833448a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "data/validation-00000-of-00001-869c898b5(â€¦):   0%|          | 0.00/9.99M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "13e31e07f6c143849ea1e014eae03d9a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/2119719 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "49e22e9f611e4af79b5c38dcbd5a8106"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/21990 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bb775d860c9e4f3cb259f6d61319b5c9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e87efce6af02435c9b82bf95143aacb2"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = UnslothTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = 2048,\n",
        "    dataset_num_proc = 8,\n",
        "\n",
        "    args = UnslothTrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 8,\n",
        "\n",
        "        warmup_ratio = 0.1,\n",
        "        num_train_epochs = 1,\n",
        "\n",
        "        learning_rate = 5e-5,\n",
        "        embedding_learning_rate = 5e-6,\n",
        "\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_torch_4bit\",\n",
        "        weight_decay = 0.00,\n",
        "        lr_scheduler_type = \"cosine\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use TrackIO/WandB etc\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "43aceb0b8dd1470ba41d22a96440505f",
            "b8b6a20ed1d749e7aedd14b06dd2965e",
            "55c0c61752134f3ba22268533d31cb7a",
            "90e58e70c06d4e66ae691b2b9ff468c3",
            "93084f534fee4f45b9a2f63ee3ef2e6d",
            "bd65edff672d4b8fb43a759e9c5a5941",
            "497cc7f4c4994db8862b623e62eb290e",
            "640fd2a32d224c168489b70ab010d987",
            "34c989e68df84afaa4c215125b075d74",
            "e1fb76a78a3446b8be8a17b044fb8e72",
            "bf4fef4449304fe2a62e7919327cc14d"
          ]
        },
        "id": "vzGcEwvfIOab",
        "outputId": "8da08d3f-ea27-49c7-9f73-86bd965fcfd9"
      },
      "id": "vzGcEwvfIOab",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=16):   0%|          | 0/2500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "43aceb0b8dd1470ba41d22a96440505f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "u39nZi0mJozd",
        "outputId": "e96cdef3-eb19-4d83-a5cf-5656909f426f"
      },
      "id": "u39nZi0mJozd",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 2,500 | Num Epochs = 1 | Total steps = 157\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 8\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 8 x 1) = 16\n",
            " \"-____-\"     Trainable parameters = 39,976,960 of 7,338,594,304 (0.54% trained)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [157/157 13:23, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.466100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.588500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.514400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.439500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.540100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.525700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.641900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.364000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.423500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.451900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.289200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.445600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.549500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.383700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.597600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.404500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>1.274900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.337400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.386200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.459800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.237700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>1.455400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>1.283400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>1.386300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.474400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>1.339600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>1.366400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.386700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>1.265900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.240600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>1.258000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>1.366700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>1.355200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>1.351200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>1.552500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>1.318300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>1.395000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>1.235900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>1.229500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.382800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>1.264900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>1.182400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>1.288300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>1.279100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>1.344700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>1.389700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>1.405100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>1.281400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>1.198500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.168100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>1.304100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>1.197100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>1.237800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>1.265600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>1.461700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>1.296800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>1.169000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>1.096200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>1.271800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.248300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>1.310900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>1.269800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>1.306500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>1.305200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>1.302100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>1.120700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>1.140300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>1.275100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>1.310100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.313100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>1.141500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>1.131200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>1.203600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>1.302200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>1.315400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>1.124100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>1.191700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>1.307800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>1.222200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.359800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>1.212200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>1.239600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>1.183500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>1.278500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>1.195200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>1.296800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>1.135200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>1.160000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>1.255500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.427000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>1.461600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>1.358400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>1.279700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>1.376600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>1.162800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>1.204000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>1.262600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>1.156100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>1.382400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.321700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101</td>\n",
              "      <td>1.245600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102</td>\n",
              "      <td>1.169800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103</td>\n",
              "      <td>1.373800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td>1.103500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>1.259400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106</td>\n",
              "      <td>1.334900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107</td>\n",
              "      <td>1.154700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108</td>\n",
              "      <td>1.318100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109</td>\n",
              "      <td>1.317300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.325800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111</td>\n",
              "      <td>1.344800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>1.229900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113</td>\n",
              "      <td>1.255100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>1.206100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>1.196300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116</td>\n",
              "      <td>1.315400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117</td>\n",
              "      <td>1.274500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118</td>\n",
              "      <td>1.095700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119</td>\n",
              "      <td>1.112500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.251900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>121</td>\n",
              "      <td>1.353700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122</td>\n",
              "      <td>1.169800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>123</td>\n",
              "      <td>1.230000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124</td>\n",
              "      <td>1.228000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>1.207900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126</td>\n",
              "      <td>1.125200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>127</td>\n",
              "      <td>1.171500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128</td>\n",
              "      <td>1.258900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>129</td>\n",
              "      <td>1.101600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>1.274000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>131</td>\n",
              "      <td>1.208500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>132</td>\n",
              "      <td>1.205300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>133</td>\n",
              "      <td>1.073500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>134</td>\n",
              "      <td>1.216100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>1.229500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>136</td>\n",
              "      <td>1.156900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>137</td>\n",
              "      <td>1.250200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>138</td>\n",
              "      <td>1.097300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>139</td>\n",
              "      <td>1.383000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.130200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>141</td>\n",
              "      <td>1.248800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>142</td>\n",
              "      <td>1.208500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>143</td>\n",
              "      <td>1.189400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>144</td>\n",
              "      <td>1.352600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145</td>\n",
              "      <td>1.257200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>146</td>\n",
              "      <td>1.351200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>147</td>\n",
              "      <td>1.359600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>148</td>\n",
              "      <td>1.188400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>149</td>\n",
              "      <td>1.177000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.289100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>151</td>\n",
              "      <td>1.331300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>152</td>\n",
              "      <td>1.326300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>153</td>\n",
              "      <td>1.185500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>154</td>\n",
              "      <td>1.265000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155</td>\n",
              "      <td>1.350600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>156</td>\n",
              "      <td>1.101100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>157</td>\n",
              "      <td>1.413300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"qlora_olmo2\")"
      ],
      "metadata": {
        "id": "qFROeK483qeX"
      },
      "id": "qFROeK483qeX",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained_merged(\"unsloth_olmo2\", tokenizer,)"
      ],
      "metadata": {
        "id": "u7Yjc_Vl3z8l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b3cb723-fc52-46ae-eff1-40a591dbeccc"
      },
      "id": "u7Yjc_Vl3z8l",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n",
            "Checking cache directory for required files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Copying 6 files from cache to `unsloth_olmo2`: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [01:12<00:00, 12.17s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully copied all 6 files from cache to `unsloth_olmo2`\n",
            "Checking cache directory for required files...\n",
            "Cache check failed: tokenizer.model not found in local cache.\n",
            "Not all required files found in cache. Will proceed with downloading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Preparing safetensor model files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 39383.14it/s]\n",
            "Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [01:51<00:00, 18.61s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Merge process complete. Saved to `/content/unsloth_olmo2`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ggml-org/llama.cpp\n"
      ],
      "metadata": {
        "id": "tCWZMELFN0Xv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bb3812c-1d23-45ba-aeeb-4a72ba9f6e86"
      },
      "id": "tCWZMELFN0Xv",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 71321, done.\u001b[K\n",
            "remote: Counting objects: 100% (292/292), done.\u001b[K\n",
            "remote: Compressing objects: 100% (197/197), done.\u001b[K\n",
            "remote: Total 71321 (delta 179), reused 95 (delta 95), pack-reused 71029 (from 2)\u001b[K\n",
            "Receiving objects: 100% (71321/71321), 230.48 MiB | 17.00 MiB/s, done.\n",
            "Resolving deltas: 100% (51446/51446), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd llama.cpp"
      ],
      "metadata": {
        "id": "WpcWRQWxPQjR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7199162c-8ce1-4249-be49-776f24318b0f"
      },
      "id": "WpcWRQWxPQjR",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/llama.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cmake -B build\n",
        "!cmake --build build --config Release"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gY_bU4hZPHb9",
        "outputId": "8fb5479d-a74f-4cc0-b189-6b7da8a757ad"
      },
      "id": "gY_bU4hZPHb9",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- The C compiler identification is GNU 11.4.0\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "\u001b[0mCMAKE_BUILD_TYPE=Release\u001b[0m\n",
            "-- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "-- The ASM compiler identification is GNU\n",
            "-- Found assembler: /usr/bin/cc\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE\n",
            "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
            "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "-- GGML_SYSTEM_ARCH: x86\n",
            "-- Including CPU backend\n",
            "-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP: TRUE (found version \"4.5\")\n",
            "-- x86 detected\n",
            "-- Adding CPU backend variant ggml-cpu: -march=native \n",
            "-- ggml version: 0.9.4\n",
            "-- ggml commit:  2fbe3b7bb\n",
            "-- Found CURL: /usr/lib/x86_64-linux-gnu/libcurl.so (found version \"7.81.0\")\n",
            "-- Configuring done (1.3s)\n",
            "-- Generating done (0.3s)\n",
            "-- Build files have been written to: /content/llama.cpp/build\n",
            "[  0%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o\u001b[0m\n",
            "[  0%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o\u001b[0m\n",
            "[  2%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-base.so\u001b[0m\n",
            "[  2%] Built target ggml-base\n",
            "[  3%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-cpu.so\u001b[0m\n",
            "[  6%] Built target ggml-cpu\n",
            "[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o\u001b[0m\n",
            "[  7%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml.so\u001b[0m\n",
            "[  7%] Built target ggml\n",
            "[  8%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-cparams.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache-iswa.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-saver.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/afmoe.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/apertus.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/arcee.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/arctic.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/arwkv7.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/baichuan.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bailingmoe.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bailingmoe2.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bert.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bitnet.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bloom.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/chameleon.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/chatglm.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/codeshell.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/cogvlm.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/cohere2-iswa.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/command-r.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/dbrx.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/deci.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/deepseek.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/deepseek2.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/dots1.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/dream.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/ernie4-5-moe.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/ernie4-5.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/exaone.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/exaone4.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/falcon-h1.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/falcon.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma-embedding.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma2-iswa.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma3.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma3n-iswa.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/glm4-moe.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/glm4.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gpt2.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gptneox.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/granite-hybrid.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/granite.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/grok.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/grovemoe.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/hunyuan-dense.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/hunyuan-moe.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/internlm2.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/jais.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/jamba.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/lfm2.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llada-moe.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llada.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llama-iswa.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llama.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mamba.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/minicpm3.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/minimax-m2.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mpt.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/nemotron-h.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/nemotron.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/neo-bert.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/olmo.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/olmo2.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/olmoe.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/openai-moe-iswa.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/openelm.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/orion.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/pangu-embedded.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/phi2.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/phi3.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plamo.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plamo2.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plm.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen2.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen2moe.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen2vl.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3vl.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3vl-moe.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3moe.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3next.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/refact.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rnd1.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv6-base.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv6.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv6qwen2.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv7-base.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv7.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/seed-oss.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/smallthinker.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/smollm3.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/stablelm.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/starcoder.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/starcoder2.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/t5-dec.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/t5-enc.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/wavtokenizer-dec.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/xverse.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mistral3.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/graph-context-mamba.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32m\u001b[1mLinking CXX shared library ../bin/libllama.so\u001b[0m\n",
            "[ 41%] Built target llama\n",
            "[ 41%] \u001b[32mBuilding CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\u001b[0m\n",
            "[ 41%] Built target build_info\n",
            "[ 41%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/arg.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-parser.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-parser-xml-toolcall.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-peg-parser.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/console.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/download.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-partial.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/llguidance.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/log.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/peg-parser.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/regex-partial.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/sampling.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/speculative.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/unicode.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32m\u001b[1mLinking CXX static library libcommon.a\u001b[0m\n",
            "[ 46%] Built target common\n",
            "[ 46%] \u001b[32mBuilding CXX object vendor/cpp-httplib/CMakeFiles/cpp-httplib.dir/httplib.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32m\u001b[1mLinking CXX static library libcpp-httplib.a\u001b[0m\n",
            "[ 46%] Built target cpp-httplib\n",
            "[ 46%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-0\u001b[0m\n",
            "[ 46%] Built target test-tokenizer-0\n",
            "[ 46%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-sampling\u001b[0m\n",
            "[ 47%] Built target test-sampling\n",
            "[ 47%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-parser\u001b[0m\n",
            "[ 47%] Built target test-grammar-parser\n",
            "[ 47%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-integration\u001b[0m\n",
            "[ 48%] Built target test-grammar-integration\n",
            "[ 48%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-llama-grammar\u001b[0m\n",
            "[ 49%] Built target test-llama-grammar\n",
            "[ 49%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat\u001b[0m\n",
            "[ 50%] Built target test-chat\n",
            "[ 50%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-schema-to-grammar\u001b[0m\n",
            "[ 51%] Built target test-json-schema-to-grammar\n",
            "[ 51%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-stats.dir/test-quantize-stats.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-stats\u001b[0m\n",
            "[ 51%] Built target test-quantize-stats\n",
            "[ 52%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gbnf-validator.dir/test-gbnf-validator.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gbnf-validator\u001b[0m\n",
            "[ 52%] Built target test-gbnf-validator\n",
            "[ 52%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-bpe\u001b[0m\n",
            "[ 53%] Built target test-tokenizer-1-bpe\n",
            "[ 53%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-spm\u001b[0m\n",
            "[ 53%] Built target test-tokenizer-1-spm\n",
            "[ 53%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-parser.dir/test-chat-parser.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-parser\u001b[0m\n",
            "[ 54%] Built target test-chat-parser\n",
            "[ 54%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-peg-parser.dir/test-chat-peg-parser.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-peg-parser.dir/peg-parser/simple-tokenize.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-peg-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-peg-parser\u001b[0m\n",
            "[ 55%] Built target test-chat-peg-parser\n",
            "[ 55%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-template\u001b[0m\n",
            "[ 55%] Built target test-chat-template\n",
            "[ 56%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-partial.dir/test-json-partial.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-partial.dir/get-model.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-partial\u001b[0m\n",
            "[ 56%] Built target test-json-partial\n",
            "[ 56%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-log\u001b[0m\n",
            "[ 57%] Built target test-log\n",
            "[ 57%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/test-peg-parser.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/simple-tokenize.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-basic.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-gbnf-generation.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-json-parser.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-json-serialization.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-unicode.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-peg-parser\u001b[0m\n",
            "[ 59%] Built target test-peg-parser\n",
            "[ 60%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-regex-partial.dir/test-regex-partial.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-regex-partial.dir/get-model.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-regex-partial\u001b[0m\n",
            "[ 60%] Built target test-regex-partial\n",
            "[ 60%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-thread-safety.dir/test-thread-safety.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-thread-safety.dir/get-model.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-thread-safety\u001b[0m\n",
            "[ 61%] Built target test-thread-safety\n",
            "[ 62%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-arg-parser\u001b[0m\n",
            "[ 62%] Built target test-arg-parser\n",
            "[ 62%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-opt.dir/test-opt.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-opt.dir/get-model.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-opt\u001b[0m\n",
            "[ 63%] Built target test-opt\n",
            "[ 63%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gguf\u001b[0m\n",
            "[ 64%] Built target test-gguf\n",
            "[ 64%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-backend-ops\u001b[0m\n",
            "[ 65%] Built target test-backend-ops\n",
            "[ 65%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-model-load-cancel\u001b[0m\n",
            "[ 65%] Built target test-model-load-cancel\n",
            "[ 65%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-autorelease\u001b[0m\n",
            "[ 66%] Built target test-autorelease\n",
            "[ 66%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-barrier\u001b[0m\n",
            "[ 66%] Built target test-barrier\n",
            "[ 67%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-fns\u001b[0m\n",
            "[ 67%] Built target test-quantize-fns\n",
            "[ 67%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-perf\u001b[0m\n",
            "[ 68%] Built target test-quantize-perf\n",
            "[ 69%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-rope\u001b[0m\n",
            "[ 69%] Built target test-rope\n",
            "[ 69%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd-audio.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/clip.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd-helper.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libmtmd.so\u001b[0m\n",
            "[ 70%] Built target mtmd\n",
            "[ 71%] \u001b[32mBuilding C object tests/CMakeFiles/test-mtmd-c-api.dir/test-mtmd-c-api.c.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-mtmd-c-api.dir/get-model.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-mtmd-c-api\u001b[0m\n",
            "[ 71%] Built target test-mtmd-c-api\n",
            "[ 72%] \u001b[32mBuilding C object tests/CMakeFiles/test-c.dir/test-c.c.o\u001b[0m\n",
            "[ 72%] \u001b[32m\u001b[1mLinking C executable ../bin/test-c\u001b[0m\n",
            "[ 72%] Built target test-c\n",
            "[ 72%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-alloc.dir/test-alloc.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-alloc.dir/get-model.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-alloc\u001b[0m\n",
            "[ 72%] Built target test-alloc\n",
            "[ 72%] \u001b[32mBuilding CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched\u001b[0m\n",
            "[ 72%] Built target llama-batched\n",
            "[ 72%] \u001b[32mBuilding CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-embedding\u001b[0m\n",
            "[ 72%] Built target llama-embedding\n",
            "[ 72%] \u001b[32mBuilding CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-eval-callback\u001b[0m\n",
            "[ 73%] Built target llama-eval-callback\n",
            "[ 74%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o\u001b[0m\n",
            "[ 74%] Built target sha256\n",
            "[ 75%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o\u001b[0m\n",
            "[ 75%] Built target xxhash\n",
            "[ 75%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o\u001b[0m\n",
            "[ 75%] Built target sha1\n",
            "[ 76%] \u001b[32mBuilding CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-hash\u001b[0m\n",
            "[ 76%] Built target llama-gguf-hash\n",
            "[ 76%] \u001b[32mBuilding CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf\u001b[0m\n",
            "[ 76%] Built target llama-gguf\n",
            "[ 77%] \u001b[32mBuilding CXX object examples/idle/CMakeFiles/llama-idle.dir/idle.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-idle\u001b[0m\n",
            "[ 77%] Built target llama-idle\n",
            "[ 77%] \u001b[32mBuilding CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookahead\u001b[0m\n",
            "[ 77%] Built target llama-lookahead\n",
            "[ 77%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup\u001b[0m\n",
            "[ 78%] Built target llama-lookup\n",
            "[ 78%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-create\u001b[0m\n",
            "[ 78%] Built target llama-lookup-create\n",
            "[ 78%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-merge\u001b[0m\n",
            "[ 79%] Built target llama-lookup-merge\n",
            "[ 79%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-stats\u001b[0m\n",
            "[ 79%] Built target llama-lookup-stats\n",
            "[ 80%] \u001b[32mBuilding CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-parallel\u001b[0m\n",
            "[ 80%] Built target llama-parallel\n",
            "[ 80%] \u001b[32mBuilding CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-passkey\u001b[0m\n",
            "[ 80%] Built target llama-passkey\n",
            "[ 81%] \u001b[32mBuilding CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-retrieval\u001b[0m\n",
            "[ 81%] Built target llama-retrieval\n",
            "[ 81%] \u001b[32mBuilding CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-save-load-state\u001b[0m\n",
            "[ 81%] Built target llama-save-load-state\n",
            "[ 81%] \u001b[32mBuilding CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple\u001b[0m\n",
            "[ 81%] Built target llama-simple\n",
            "[ 82%] \u001b[32mBuilding CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple-chat\u001b[0m\n",
            "[ 82%] Built target llama-simple-chat\n",
            "[ 82%] \u001b[32mBuilding CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative\u001b[0m\n",
            "[ 83%] Built target llama-speculative\n",
            "[ 83%] \u001b[32mBuilding CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative-simple\u001b[0m\n",
            "[ 83%] Built target llama-speculative-simple\n",
            "[ 84%] \u001b[32mBuilding CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gen-docs\u001b[0m\n",
            "[ 84%] Built target llama-gen-docs\n",
            "[ 84%] \u001b[32mBuilding CXX object examples/training/CMakeFiles/llama-finetune.dir/finetune.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-finetune\u001b[0m\n",
            "[ 85%] Built target llama-finetune\n",
            "[ 85%] \u001b[32mBuilding CXX object examples/diffusion/CMakeFiles/llama-diffusion-cli.dir/diffusion-cli.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-diffusion-cli\u001b[0m\n",
            "[ 86%] Built target llama-diffusion-cli\n",
            "[ 86%] \u001b[32mBuilding CXX object examples/model-conversion/CMakeFiles/llama-logits.dir/logits.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-logits\u001b[0m\n",
            "[ 87%] Built target llama-logits\n",
            "[ 87%] \u001b[32mBuilding CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-convert-llama2c-to-ggml\u001b[0m\n",
            "[ 88%] Built target llama-convert-llama2c-to-ggml\n",
            "[ 88%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-vdot\u001b[0m\n",
            "[ 89%] Built target llama-vdot\n",
            "[ 89%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-q8dot\u001b[0m\n",
            "[ 89%] Built target llama-q8dot\n",
            "[ 90%] \u001b[32mBuilding CXX object tools/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched-bench\u001b[0m\n",
            "[ 90%] Built target llama-batched-bench\n",
            "[ 90%] \u001b[32mBuilding CXX object tools/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-split\u001b[0m\n",
            "[ 90%] Built target llama-gguf-split\n",
            "[ 90%] \u001b[32mBuilding CXX object tools/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-imatrix\u001b[0m\n",
            "[ 90%] Built target llama-imatrix\n",
            "[ 90%] \u001b[32mBuilding CXX object tools/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-bench\u001b[0m\n",
            "[ 91%] Built target llama-bench\n",
            "[ 91%] \u001b[32mBuilding CXX object tools/main/CMakeFiles/llama-cli.dir/main.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cli\u001b[0m\n",
            "[ 91%] Built target llama-cli\n",
            "[ 92%] \u001b[32mBuilding CXX object tools/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-perplexity\u001b[0m\n",
            "[ 92%] Built target llama-perplexity\n",
            "[ 93%] \u001b[32mBuilding CXX object tools/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize\u001b[0m\n",
            "[ 93%] Built target llama-quantize\n",
            "[ 93%] \u001b[34m\u001b[1mGenerating loading.html.hpp\u001b[0m\n",
            "[ 94%] \u001b[34m\u001b[1mGenerating index.html.gz.hpp\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-http.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-models.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-task.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-queue.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-common.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-context.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-server\u001b[0m\n",
            "[ 96%] Built target llama-server\n",
            "[ 96%] \u001b[32mBuilding CXX object tools/run/CMakeFiles/llama-run.dir/run.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CXX object tools/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-run\u001b[0m\n",
            "[ 97%] Built target llama-run\n",
            "[ 97%] \u001b[32mBuilding CXX object tools/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tokenize\u001b[0m\n",
            "[ 98%] Built target llama-tokenize\n",
            "[ 98%] \u001b[32mBuilding CXX object tools/tts/CMakeFiles/llama-tts.dir/tts.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tts\u001b[0m\n",
            "[ 98%] Built target llama-tts\n",
            "[ 99%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-llava-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[ 99%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-llava-cli\u001b[0m\n",
            "[ 99%] Built target llama-llava-cli\n",
            "[ 99%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-gemma3-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[ 99%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gemma3-cli\u001b[0m\n",
            "[ 99%] Built target llama-gemma3-cli\n",
            "[ 99%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-minicpmv-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-minicpmv-cli\u001b[0m\n",
            "[100%] Built target llama-minicpmv-cli\n",
            "[100%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-qwen2vl-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-qwen2vl-cli\u001b[0m\n",
            "[100%] Built target llama-qwen2vl-cli\n",
            "[100%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-mtmd-cli.dir/mtmd-cli.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-mtmd-cli\u001b[0m\n",
            "[100%] Built target llama-mtmd-cli\n",
            "[100%] \u001b[32mBuilding CXX object tools/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cvector-generator\u001b[0m\n",
            "[100%] Built target llama-cvector-generator\n",
            "[100%] \u001b[32mBuilding CXX object tools/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-export-lora\u001b[0m\n",
            "[100%] Built target llama-export-lora\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 convert_hf_to_gguf.py ../unsloth_olmo2 --outfile olmo2_quantized.gguf --outtype q8_0"
      ],
      "metadata": {
        "id": "BAcZK9Mr6QUx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ace3f86a-59da-4f30-ebdb-94c6cc7a4179"
      },
      "id": "BAcZK9Mr6QUx",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:hf-to-gguf:Loading model: unsloth_olmo2\n",
            "INFO:hf-to-gguf:Model architecture: Olmo2ForCausalLM\n",
            "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
            "INFO:hf-to-gguf:gguf: indexing model part 'model-00003-of-00006.safetensors'\n",
            "INFO:hf-to-gguf:gguf: indexing model part 'model-00004-of-00006.safetensors'\n",
            "INFO:hf-to-gguf:gguf: indexing model part 'model-00005-of-00006.safetensors'\n",
            "INFO:hf-to-gguf:gguf: indexing model part 'model-00006-of-00006.safetensors'\n",
            "INFO:hf-to-gguf:gguf: indexing model part 'model-00002-of-00006.safetensors'\n",
            "INFO:hf-to-gguf:gguf: indexing model part 'model-00001-of-00006.safetensors'\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,            torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.10.ffn_gate.weight,            torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,              torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.10.post_attention_norm.weight, torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.10.post_ffw_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_k_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_q_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,            torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.11.ffn_gate.weight,            torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,              torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.11.post_attention_norm.weight, torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.11.post_ffw_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_k_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_q_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight,            torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.12.ffn_gate.weight,            torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight,              torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.12.post_attention_norm.weight, torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.12.post_ffw_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_k_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_q_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight,            torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.13.ffn_gate.weight,            torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight,              torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.13.post_attention_norm.weight, torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.13.post_ffw_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_k_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_q_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight,            torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.14.ffn_gate.weight,            torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight,              torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.14.post_attention_norm.weight, torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.14.post_ffw_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_k_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_q_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight,            torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.15.ffn_gate.weight,            torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight,              torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.15.post_attention_norm.weight, torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.15.post_ffw_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_k_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_q_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_k_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_output.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_q_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.16.ffn_down.weight,            torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.16.ffn_gate.weight,            torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.16.ffn_up.weight,              torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.16.post_attention_norm.weight, torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.16.post_ffw_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.17.ffn_down.weight,            torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.17.ffn_gate.weight,            torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.17.ffn_up.weight,              torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.17.post_attention_norm.weight, torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.17.post_ffw_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_k_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_output.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_q_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.18.ffn_down.weight,            torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.18.ffn_gate.weight,            torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.18.ffn_up.weight,              torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.18.post_attention_norm.weight, torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.18.post_ffw_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_k_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_output.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_q_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.19.ffn_down.weight,            torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.19.ffn_gate.weight,            torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.19.ffn_up.weight,              torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.19.post_attention_norm.weight, torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.19.post_ffw_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_k_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_output.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_q_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.20.ffn_down.weight,            torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.20.ffn_gate.weight,            torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.20.ffn_up.weight,              torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.20.post_attention_norm.weight, torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.20.post_ffw_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.20.attn_k_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.20.attn_output.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.20.attn_q_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.21.ffn_down.weight,            torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.21.ffn_gate.weight,            torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.21.ffn_up.weight,              torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.21.post_attention_norm.weight, torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.21.post_ffw_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_k_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_output.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_q_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_k_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_output.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_q_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.22.ffn_down.weight,            torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.22.ffn_gate.weight,            torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.22.ffn_up.weight,              torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.22.post_attention_norm.weight, torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.22.post_ffw_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.23.ffn_down.weight,            torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.23.ffn_gate.weight,            torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.23.ffn_up.weight,              torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.23.post_attention_norm.weight, torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.23.post_ffw_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_k_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_output.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_q_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.24.ffn_down.weight,            torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.24.ffn_gate.weight,            torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.24.ffn_up.weight,              torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.24.post_attention_norm.weight, torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.24.post_ffw_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.24.attn_k_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.24.attn_k.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.24.attn_output.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.24.attn_q_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.24.attn_q.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.24.attn_v.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.25.ffn_down.weight,            torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.25.ffn_gate.weight,            torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.25.ffn_up.weight,              torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.25.post_attention_norm.weight, torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.25.post_ffw_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.25.attn_k_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.25.attn_k.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.25.attn_output.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.25.attn_q_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.25.attn_q.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.25.attn_v.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.26.ffn_down.weight,            torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.26.ffn_gate.weight,            torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.26.ffn_up.weight,              torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.26.post_attention_norm.weight, torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.26.post_ffw_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.26.attn_k_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.26.attn_k.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.26.attn_output.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.26.attn_q_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.26.attn_q.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.26.attn_v.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.27.ffn_down.weight,            torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.27.ffn_gate.weight,            torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.27.ffn_up.weight,              torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.27.post_attention_norm.weight, torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.27.post_ffw_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.27.attn_k_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.27.attn_k.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.27.attn_output.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.27.attn_q_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.27.attn_q.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.27.attn_v.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.28.attn_k_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.28.attn_k.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.28.attn_output.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.28.attn_q_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.28.attn_q.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.28.attn_v.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:output.weight,                     torch.float32 --> Q8_0, shape = {4096, 100352}\n",
            "INFO:hf-to-gguf:blk.28.ffn_down.weight,            torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.28.ffn_gate.weight,            torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.28.ffn_up.weight,              torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.28.post_attention_norm.weight, torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.28.post_ffw_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.29.ffn_down.weight,            torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.29.ffn_gate.weight,            torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.29.ffn_up.weight,              torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.29.post_attention_norm.weight, torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.29.post_ffw_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.29.attn_k_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.29.attn_k.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.29.attn_output.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.29.attn_q_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.29.attn_q.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.29.attn_v.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.30.ffn_down.weight,            torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.30.ffn_gate.weight,            torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.30.ffn_up.weight,              torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.30.post_attention_norm.weight, torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.30.post_ffw_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.30.attn_k_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.30.attn_k.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.30.attn_output.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.30.attn_q_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.30.attn_q.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.30.attn_v.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.31.ffn_down.weight,            torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.31.ffn_gate.weight,            torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.31.ffn_up.weight,              torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.31.post_attention_norm.weight, torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.31.post_ffw_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.31.attn_k_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.31.attn_k.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.31.attn_output.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.31.attn_q_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.31.attn_q.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.31.attn_v.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:output_norm.weight,                torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,             torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.4.ffn_gate.weight,             torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,               torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.4.post_attention_norm.weight,  torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.4.post_ffw_norm.weight,        torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_k_norm.weight,          torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,          torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_q_norm.weight,          torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,             torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.5.ffn_gate.weight,             torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,               torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.5.post_attention_norm.weight,  torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.5.post_ffw_norm.weight,        torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_k_norm.weight,          torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,          torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_q_norm.weight,          torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,             torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.6.ffn_gate.weight,             torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,               torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.6.post_attention_norm.weight,  torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.6.post_ffw_norm.weight,        torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_k_norm.weight,          torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,          torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_q_norm.weight,          torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,             torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.7.ffn_gate.weight,             torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,               torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.7.post_attention_norm.weight,  torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.7.post_ffw_norm.weight,        torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_k_norm.weight,          torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,          torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_q_norm.weight,          torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,             torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.8.ffn_gate.weight,             torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,               torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.8.post_attention_norm.weight,  torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.8.post_ffw_norm.weight,        torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_k_norm.weight,          torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,          torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_q_norm.weight,          torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,             torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.9.ffn_gate.weight,             torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,               torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.9.post_attention_norm.weight,  torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.9.post_ffw_norm.weight,        torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_k_norm.weight,          torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,          torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_q_norm.weight,          torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:token_embd.weight,                 torch.float32 --> Q8_0, shape = {4096, 100352}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,             torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.0.ffn_gate.weight,             torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,               torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.0.post_attention_norm.weight,  torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.0.post_ffw_norm.weight,        torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.0.attn_k_norm.weight,          torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,          torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.0.attn_q_norm.weight,          torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,             torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.1.ffn_gate.weight,             torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,               torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.1.post_attention_norm.weight,  torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.1.post_ffw_norm.weight,        torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_k_norm.weight,          torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,          torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_q_norm.weight,          torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,             torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.2.ffn_gate.weight,             torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,               torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.2.post_attention_norm.weight,  torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.2.post_ffw_norm.weight,        torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_k_norm.weight,          torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,          torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_q_norm.weight,          torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,             torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.3.ffn_gate.weight,             torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,               torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.3.post_attention_norm.weight,  torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.3.post_ffw_norm.weight,        torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_k_norm.weight,          torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,          torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_q_norm.weight,          torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:Set meta model\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:gguf: context length = 4096\n",
            "INFO:hf-to-gguf:gguf: embedding length = 4096\n",
            "INFO:hf-to-gguf:gguf: feed forward length = 11008\n",
            "INFO:hf-to-gguf:gguf: head count = 32\n",
            "INFO:hf-to-gguf:gguf: key-value head count = 32\n",
            "INFO:hf-to-gguf:gguf: rope theta = 500000\n",
            "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\n",
            "INFO:hf-to-gguf:gguf: file type = 7\n",
            "INFO:hf-to-gguf:Set model quantization version\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "INFO:numexpr.utils:NumExpr defaulting to 12 threads.\n",
            "INFO:gguf.vocab:Adding 100000 merge(s).\n",
            "INFO:gguf.vocab:Setting special token type bos to 100257\n",
            "INFO:gguf.vocab:Setting special token type eos to 100257\n",
            "INFO:gguf.vocab:Setting special token type unk to 100257\n",
            "INFO:gguf.vocab:Setting special token type pad to 100277\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:olmo2_quantized.gguf: n_tensors = 355, total_size = 7.8G\n",
            "Writing: 100% 7.76G/7.76G [01:27<00:00, 88.5Mbyte/s]\n",
            "INFO:hf-to-gguf:Model successfully exported to olmo2_quantized.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('olmo2_quantized.gguf')"
      ],
      "metadata": {
        "id": "_fzqWzXPWHP_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "711b1484-d78b-4ab8-f00f-8c8fa5de4a46"
      },
      "id": "_fzqWzXPWHP_",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a2608e15-0812-44b1-91a4-2730bf4725a9\", \"olmo2_quantized.gguf\", 7759894496)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}