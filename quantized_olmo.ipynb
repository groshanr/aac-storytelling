{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4VQhQ763zMFS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VQhQ763zMFS",
        "outputId": "057eef50-6702-4cae-da47-0e5720a0d725"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: unsloth in /usr/local/lib/python3.12/dist-packages (2025.11.6)\n",
            "Requirement already satisfied: unsloth_zoo>=2025.11.6 in /usr/local/lib/python3.12/dist-packages (from unsloth) (2025.11.6)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.45.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from unsloth) (25.0)\n",
            "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.24.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from unsloth) (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from unsloth) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from unsloth) (5.9.5)\n",
            "Requirement already satisfied: tyro in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.9.35)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from unsloth) (5.29.5)\n",
            "Requirement already satisfied: xformers>=0.0.27.post2 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.0.33.post1)\n",
            "Requirement already satisfied: bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.48.2)\n",
            "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (3.5.0)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.2.1)\n",
            "Requirement already satisfied: datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1 in /usr/local/lib/python3.12/dist-packages (from unsloth) (4.3.0)\n",
            "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.12/dist-packages (from unsloth) (1.12.0)\n",
            "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.18.0)\n",
            "Requirement already satisfied: huggingface_hub>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.36.0)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.1.9)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.35.2)\n",
            "Requirement already satisfied: transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.2,>=4.51.3 in /usr/local/lib/python3.12/dist-packages (from unsloth) (4.57.1)\n",
            "Requirement already satisfied: trl!=0.19.0,<=0.24.0,>=0.18.2 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.24.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.34.1->unsloth) (6.0.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.34.1->unsloth) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (22.0.0)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.32.4)\n",
            "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.28.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0->unsloth) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0->unsloth) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (1.11.1.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.2,>=4.51.3->unsloth) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.2,>=4.51.3->unsloth) (0.22.1)\n",
            "Requirement already satisfied: torchao>=0.13.0 in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2025.11.6->unsloth) (0.14.1)\n",
            "Requirement already satisfied: cut_cross_entropy in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2025.11.6->unsloth) (25.1.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2025.11.6->unsloth) (11.3.0)\n",
            "Requirement already satisfied: msgspec in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2025.11.6->unsloth) (0.20.0)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers->unsloth) (8.7.0)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (0.17.0)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (13.9.4)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (1.8.0)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (4.4.4)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.13.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.5.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro->unsloth) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro->unsloth) (2.19.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.4.0->unsloth) (1.3.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers->unsloth) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.22.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.17.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2_4R-kLtN1qT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_4R-kLtN1qT",
        "outputId": "20464935-c470-4c73-a34b-2be9a3d56aba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==4.57.1 in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.57.1) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.57.1) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.57.1) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.57.1) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.57.1) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.57.1) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.57.1) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.57.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93d6b3ac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93d6b3ac",
        "outputId": "75c37aae-eae0-4030-df53-ab5288a3cd5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel, FastModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "W7yvF708yILP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159,
          "referenced_widgets": [
            "3c267d2a38314955bdb4bc662aee0324",
            "a637fb902e4d44ea959f85508bce6ae5",
            "c82194df3eaf489ead35feecb08b3616",
            "110d3945241347eab1d81aa8f2c0d1ee",
            "40495a2f13f04658bb2cf5115f35405b",
            "b8fb094609fb4682a946f7996003cc41",
            "fca2b896ea914c70b62ac450a321c08d",
            "ce477428b3d34a3ca027dd2cfd84d1c3",
            "d7eb1cce81d04176873476c4e0bb3e83",
            "cddfbe15907040cda904d7e31c1572f4",
            "70492a5030664e5c8e43ecc04b3c19ef"
          ]
        },
        "id": "W7yvF708yILP",
        "outputId": "278c1815-1f40-4dfd-8c6a-d13431f9c1e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2025.11.6: Fast Olmo2 patching. Transformers: 4.57.1.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.318 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 8.0. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post1. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c267d2a38314955bdb4bc662aee0324",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name = \"allenai/OLMo-2-1124-7B\",\n",
        "    max_seq_length = 8092,\n",
        "    load_in_4bit = True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cb0807d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cb0807d",
        "outputId": "13101a00-f622-4882-866e-d8bd76e8e78d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Making `model.base_model.model.model` require gradients\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'],\n",
        "    lora_alpha = 32,\n",
        "    qat_scheme = \"int4\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qFROeK483qeX",
      "metadata": {
        "id": "qFROeK483qeX"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"qlora_olmo2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u7Yjc_Vl3z8l",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7Yjc_Vl3z8l",
        "outputId": "469e6ed8-3707-41f2-e527-0c75e64fea18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n",
            "Checking cache directory for required files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Copying 6 files from cache to `unsloth_olmo2`: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [01:11<00:00, 11.86s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully copied all 6 files from cache to `unsloth_olmo2`\n",
            "Checking cache directory for required files...\n",
            "Cache check failed: tokenizer.model not found in local cache.\n",
            "Not all required files found in cache. Will proceed with downloading.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Preparing safetensor model files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 46006.99it/s]\n",
            "Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [01:57<00:00, 19.53s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Merge process complete. Saved to `/content/unsloth_olmo2`\n"
          ]
        }
      ],
      "source": [
        "model.save_pretrained_merged(\"unsloth_olmo2\", tokenizer,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tCWZMELFN0Xv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCWZMELFN0Xv",
        "outputId": "e348be92-f33e-4ab8-efd6-92e7719ece6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'llama.cpp' already exists and is not an empty directory.\n",
            "/content/llama.cpp\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ggml-org/llama.cpp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WpcWRQWxPQjR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpcWRQWxPQjR",
        "outputId": "3e56583c-f799-44ce-b39e-bf315b3c3f9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'llama.cpp'\n",
            "/content/llama.cpp\n"
          ]
        }
      ],
      "source": [
        "%cd llama.cpp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gY_bU4hZPHb9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gY_bU4hZPHb9",
        "outputId": "21a0774a-1b3c-4e9a-c650-ff03f773a86c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- The C compiler identification is GNU 11.4.0\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "\u001b[0mCMAKE_BUILD_TYPE=Release\u001b[0m\n",
            "-- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "-- The ASM compiler identification is GNU\n",
            "-- Found assembler: /usr/bin/cc\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE\n",
            "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
            "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "-- GGML_SYSTEM_ARCH: x86\n",
            "-- Including CPU backend\n",
            "-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP: TRUE (found version \"4.5\")\n",
            "-- x86 detected\n",
            "-- Adding CPU backend variant ggml-cpu: -march=native \n",
            "-- ggml version: 0.9.4\n",
            "-- ggml commit:  16cc3c606\n",
            "-- Found CURL: /usr/lib/x86_64-linux-gnu/libcurl.so (found version \"7.81.0\")\n",
            "-- Configuring done (1.7s)\n",
            "-- Generating done (0.3s)\n",
            "-- Build files have been written to: /content/llama.cpp/build\n",
            "[  1%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o\u001b[0m\n",
            "[  3%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-base.so\u001b[0m\n",
            "[  3%] Built target ggml-base\n",
            "[  3%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o\u001b[0m\n",
            "[  7%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-cpu.so\u001b[0m\n",
            "[  7%] Built target ggml-cpu\n",
            "[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o\u001b[0m\n",
            "[  7%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml.so\u001b[0m\n",
            "[  7%] Built target ggml\n",
            "[  8%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-cparams.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache-iswa.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-saver.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/afmoe.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/apertus.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/arcee.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/arctic.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/arwkv7.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/baichuan.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bailingmoe.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bailingmoe2.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bert.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bitnet.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bloom.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/chameleon.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/chatglm.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/codeshell.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/cogvlm.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/cohere2-iswa.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/command-r.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/dbrx.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/deci.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/deepseek.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/deepseek2.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/dots1.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/dream.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/ernie4-5-moe.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/ernie4-5.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/exaone.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/exaone4.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/falcon-h1.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/falcon.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma-embedding.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma2-iswa.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma3-iswa.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma3n-iswa.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/glm4-moe.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/glm4.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gpt2.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gptneox.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/granite-hybrid.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/granite.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/grok.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/grovemoe.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/hunyuan-dense.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/hunyuan-moe.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/internlm2.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/jais.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/jamba.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/lfm2.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llada-moe.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llada.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llama-iswa.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llama.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mamba.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/minicpm3.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/minimax-m2.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mpt.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/nemotron-h.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/nemotron.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/neo-bert.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/olmo.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/olmo2.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/olmoe.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/openai-moe-iswa.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/openelm.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/orion.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/pangu-embedded.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/phi2.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/phi3.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plamo.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plamo2.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plm.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen2.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen2moe.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen2vl.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3vl.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3vl-moe.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3moe.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3next.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/refact.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rnd1.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv6-base.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv6.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv6qwen2.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv7-base.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv7.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/seed-oss.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/smallthinker.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/smollm3.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/stablelm.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/starcoder.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/starcoder2.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/t5-dec.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/t5-enc.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/wavtokenizer-dec.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/xverse.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mistral3.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/graph-context-mamba.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32m\u001b[1mLinking CXX shared library ../bin/libllama.so\u001b[0m\n",
            "[ 42%] Built target llama\n",
            "[ 42%] \u001b[32mBuilding CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\u001b[0m\n",
            "[ 42%] Built target build_info\n",
            "[ 42%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/arg.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-parser.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-parser-xml-toolcall.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/console.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/download.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-partial.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/llguidance.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/log.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/regex-partial.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/sampling.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/speculative.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32m\u001b[1mLinking CXX static library libcommon.a\u001b[0m\n",
            "[ 46%] Built target common\n",
            "[ 47%] \u001b[32mBuilding CXX object vendor/cpp-httplib/CMakeFiles/cpp-httplib.dir/httplib.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32m\u001b[1mLinking CXX static library libcpp-httplib.a\u001b[0m\n",
            "[ 47%] Built target cpp-httplib\n",
            "[ 47%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-0\u001b[0m\n",
            "[ 47%] Built target test-tokenizer-0\n",
            "[ 47%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-sampling\u001b[0m\n",
            "[ 48%] Built target test-sampling\n",
            "[ 49%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-parser\u001b[0m\n",
            "[ 49%] Built target test-grammar-parser\n",
            "[ 50%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-integration\u001b[0m\n",
            "[ 50%] Built target test-grammar-integration\n",
            "[ 50%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-llama-grammar\u001b[0m\n",
            "[ 51%] Built target test-llama-grammar\n",
            "[ 51%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat\u001b[0m\n",
            "[ 51%] Built target test-chat\n",
            "[ 51%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-schema-to-grammar\u001b[0m\n",
            "[ 52%] Built target test-json-schema-to-grammar\n",
            "[ 52%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-stats.dir/test-quantize-stats.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-stats\u001b[0m\n",
            "[ 52%] Built target test-quantize-stats\n",
            "[ 52%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gbnf-validator.dir/test-gbnf-validator.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gbnf-validator\u001b[0m\n",
            "[ 53%] Built target test-gbnf-validator\n",
            "[ 53%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-bpe\u001b[0m\n",
            "[ 54%] Built target test-tokenizer-1-bpe\n",
            "[ 54%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-spm\u001b[0m\n",
            "[ 54%] Built target test-tokenizer-1-spm\n",
            "[ 55%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-parser.dir/test-chat-parser.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-parser\u001b[0m\n",
            "[ 55%] Built target test-chat-parser\n",
            "[ 56%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-template\u001b[0m\n",
            "[ 56%] Built target test-chat-template\n",
            "[ 56%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-partial.dir/test-json-partial.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-partial.dir/get-model.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-partial\u001b[0m\n",
            "[ 57%] Built target test-json-partial\n",
            "[ 57%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-log\u001b[0m\n",
            "[ 57%] Built target test-log\n",
            "[ 57%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-regex-partial.dir/test-regex-partial.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-regex-partial.dir/get-model.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-regex-partial\u001b[0m\n",
            "[ 58%] Built target test-regex-partial\n",
            "[ 58%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-thread-safety.dir/test-thread-safety.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-thread-safety.dir/get-model.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-thread-safety\u001b[0m\n",
            "[ 59%] Built target test-thread-safety\n",
            "[ 59%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-arg-parser\u001b[0m\n",
            "[ 59%] Built target test-arg-parser\n",
            "[ 59%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-opt.dir/test-opt.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-opt.dir/get-model.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-opt\u001b[0m\n",
            "[ 60%] Built target test-opt\n",
            "[ 60%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gguf\u001b[0m\n",
            "[ 60%] Built target test-gguf\n",
            "[ 61%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-backend-ops\u001b[0m\n",
            "[ 61%] Built target test-backend-ops\n",
            "[ 62%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-model-load-cancel\u001b[0m\n",
            "[ 62%] Built target test-model-load-cancel\n",
            "[ 63%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-autorelease\u001b[0m\n",
            "[ 63%] Built target test-autorelease\n",
            "[ 63%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-barrier\u001b[0m\n",
            "[ 64%] Built target test-barrier\n",
            "[ 64%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-fns\u001b[0m\n",
            "[ 65%] Built target test-quantize-fns\n",
            "[ 65%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-perf\u001b[0m\n",
            "[ 66%] Built target test-quantize-perf\n",
            "[ 66%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-rope\u001b[0m\n",
            "[ 67%] Built target test-rope\n",
            "[ 67%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd-audio.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/clip.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd-helper.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libmtmd.so\u001b[0m\n",
            "[ 68%] Built target mtmd\n",
            "[ 69%] \u001b[32mBuilding C object tests/CMakeFiles/test-mtmd-c-api.dir/test-mtmd-c-api.c.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-mtmd-c-api.dir/get-model.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-mtmd-c-api\u001b[0m\n",
            "[ 69%] Built target test-mtmd-c-api\n",
            "[ 69%] \u001b[32mBuilding C object tests/CMakeFiles/test-c.dir/test-c.c.o\u001b[0m\n",
            "[ 70%] \u001b[32m\u001b[1mLinking C executable ../bin/test-c\u001b[0m\n",
            "[ 70%] Built target test-c\n",
            "[ 70%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-alloc.dir/test-alloc.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-alloc.dir/get-model.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-alloc\u001b[0m\n",
            "[ 71%] Built target test-alloc\n",
            "[ 72%] \u001b[32mBuilding CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched\u001b[0m\n",
            "[ 72%] Built target llama-batched\n",
            "[ 73%] \u001b[32mBuilding CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-embedding\u001b[0m\n",
            "[ 73%] Built target llama-embedding\n",
            "[ 73%] \u001b[32mBuilding CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-eval-callback\u001b[0m\n",
            "[ 73%] Built target llama-eval-callback\n",
            "[ 73%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o\u001b[0m\n",
            "[ 73%] Built target sha256\n",
            "[ 74%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o\u001b[0m\n",
            "[ 74%] Built target xxhash\n",
            "[ 75%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o\u001b[0m\n",
            "[ 75%] Built target sha1\n",
            "[ 75%] \u001b[32mBuilding CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-hash\u001b[0m\n",
            "[ 76%] Built target llama-gguf-hash\n",
            "[ 76%] \u001b[32mBuilding CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf\u001b[0m\n",
            "[ 76%] Built target llama-gguf\n",
            "[ 76%] \u001b[32mBuilding CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookahead\u001b[0m\n",
            "[ 77%] Built target llama-lookahead\n",
            "[ 77%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup\u001b[0m\n",
            "[ 77%] Built target llama-lookup\n",
            "[ 77%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-create\u001b[0m\n",
            "[ 78%] Built target llama-lookup-create\n",
            "[ 78%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-merge\u001b[0m\n",
            "[ 78%] Built target llama-lookup-merge\n",
            "[ 79%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-stats\u001b[0m\n",
            "[ 79%] Built target llama-lookup-stats\n",
            "[ 79%] \u001b[32mBuilding CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-parallel\u001b[0m\n",
            "[ 79%] Built target llama-parallel\n",
            "[ 80%] \u001b[32mBuilding CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-passkey\u001b[0m\n",
            "[ 80%] Built target llama-passkey\n",
            "[ 81%] \u001b[32mBuilding CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-retrieval\u001b[0m\n",
            "[ 81%] Built target llama-retrieval\n",
            "[ 81%] \u001b[32mBuilding CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-save-load-state\u001b[0m\n",
            "[ 81%] Built target llama-save-load-state\n",
            "[ 81%] \u001b[32mBuilding CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple\u001b[0m\n",
            "[ 82%] Built target llama-simple\n",
            "[ 82%] \u001b[32mBuilding CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple-chat\u001b[0m\n",
            "[ 82%] Built target llama-simple-chat\n",
            "[ 82%] \u001b[32mBuilding CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative\u001b[0m\n",
            "[ 83%] Built target llama-speculative\n",
            "[ 83%] \u001b[32mBuilding CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative-simple\u001b[0m\n",
            "[ 83%] Built target llama-speculative-simple\n",
            "[ 83%] \u001b[32mBuilding CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gen-docs\u001b[0m\n",
            "[ 84%] Built target llama-gen-docs\n",
            "[ 84%] \u001b[32mBuilding CXX object examples/training/CMakeFiles/llama-finetune.dir/finetune.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-finetune\u001b[0m\n",
            "[ 85%] Built target llama-finetune\n",
            "[ 85%] \u001b[32mBuilding CXX object examples/diffusion/CMakeFiles/llama-diffusion-cli.dir/diffusion-cli.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-diffusion-cli\u001b[0m\n",
            "[ 85%] Built target llama-diffusion-cli\n",
            "[ 86%] \u001b[32mBuilding CXX object examples/model-conversion/CMakeFiles/llama-logits.dir/logits.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-logits\u001b[0m\n",
            "[ 86%] Built target llama-logits\n",
            "[ 86%] \u001b[32mBuilding CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-convert-llama2c-to-ggml\u001b[0m\n",
            "[ 86%] Built target llama-convert-llama2c-to-ggml\n",
            "[ 87%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-vdot\u001b[0m\n",
            "[ 87%] Built target llama-vdot\n",
            "[ 87%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-q8dot\u001b[0m\n",
            "[ 87%] Built target llama-q8dot\n",
            "[ 87%] \u001b[32mBuilding CXX object tools/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched-bench\u001b[0m\n",
            "[ 88%] Built target llama-batched-bench\n",
            "[ 88%] \u001b[32mBuilding CXX object tools/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-split\u001b[0m\n",
            "[ 88%] Built target llama-gguf-split\n",
            "[ 89%] \u001b[32mBuilding CXX object tools/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-imatrix\u001b[0m\n",
            "[ 89%] Built target llama-imatrix\n",
            "[ 89%] \u001b[32mBuilding CXX object tools/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-bench\u001b[0m\n",
            "[ 89%] Built target llama-bench\n",
            "[ 89%] \u001b[32mBuilding CXX object tools/main/CMakeFiles/llama-cli.dir/main.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cli\u001b[0m\n",
            "[ 90%] Built target llama-cli\n",
            "[ 90%] \u001b[32mBuilding CXX object tools/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-perplexity\u001b[0m\n",
            "[ 91%] Built target llama-perplexity\n",
            "[ 91%] \u001b[32mBuilding CXX object tools/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize\u001b[0m\n",
            "[ 92%] Built target llama-quantize\n",
            "[ 93%] \u001b[34m\u001b[1mGenerating loading.html.hpp\u001b[0m\n",
            "[ 93%] \u001b[34m\u001b[1mGenerating index.html.gz.hpp\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-http.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-models.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-task.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-queue.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-common.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-context.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-server\u001b[0m\n",
            "[ 95%] Built target llama-server\n",
            "[ 95%] \u001b[32mBuilding CXX object tools/run/CMakeFiles/llama-run.dir/run.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object tools/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-run\u001b[0m\n",
            "[ 96%] Built target llama-run\n",
            "[ 96%] \u001b[32mBuilding CXX object tools/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tokenize\u001b[0m\n",
            "[ 97%] Built target llama-tokenize\n",
            "[ 97%] \u001b[32mBuilding CXX object tools/tts/CMakeFiles/llama-tts.dir/tts.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tts\u001b[0m\n",
            "[ 97%] Built target llama-tts\n",
            "[ 97%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-llava-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-llava-cli\u001b[0m\n",
            "[ 97%] Built target llama-llava-cli\n",
            "[ 97%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-gemma3-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gemma3-cli\u001b[0m\n",
            "[ 97%] Built target llama-gemma3-cli\n",
            "[ 97%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-minicpmv-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-minicpmv-cli\u001b[0m\n",
            "[ 97%] Built target llama-minicpmv-cli\n",
            "[ 97%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-qwen2vl-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-qwen2vl-cli\u001b[0m\n",
            "[ 97%] Built target llama-qwen2vl-cli\n",
            "[ 98%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-mtmd-cli.dir/mtmd-cli.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-mtmd-cli\u001b[0m\n",
            "[ 98%] Built target llama-mtmd-cli\n",
            "[ 98%] \u001b[32mBuilding CXX object tools/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o\u001b[0m\n",
            "[ 99%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cvector-generator\u001b[0m\n",
            "[ 99%] Built target llama-cvector-generator\n",
            "[100%] \u001b[32mBuilding CXX object tools/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-export-lora\u001b[0m\n",
            "[100%] Built target llama-export-lora\n"
          ]
        }
      ],
      "source": [
        "!cmake -B build\n",
        "!cmake --build build --config Release"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BAcZK9Mr6QUx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAcZK9Mr6QUx",
        "outputId": "7521ea2f-d717-485c-8e49-9296aece2fc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:hf-to-gguf:Loading model: unsloth_olmo2\n",
            "INFO:hf-to-gguf:Model architecture: Olmo2ForCausalLM\n",
            "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
            "INFO:hf-to-gguf:gguf: indexing model part 'model-00001-of-00006.safetensors'\n",
            "INFO:hf-to-gguf:gguf: indexing model part 'model-00003-of-00006.safetensors'\n",
            "INFO:hf-to-gguf:gguf: indexing model part 'model-00005-of-00006.safetensors'\n",
            "INFO:hf-to-gguf:gguf: indexing model part 'model-00004-of-00006.safetensors'\n",
            "INFO:hf-to-gguf:gguf: indexing model part 'model-00002-of-00006.safetensors'\n",
            "INFO:hf-to-gguf:gguf: indexing model part 'model-00006-of-00006.safetensors'\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:token_embd.weight,                 torch.float32 --> Q8_0, shape = {4096, 100352}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,             torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.0.ffn_gate.weight,             torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,               torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.0.post_attention_norm.weight,  torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.0.post_ffw_norm.weight,        torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.0.attn_k_norm.weight,          torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,          torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.0.attn_q_norm.weight,          torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,             torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.1.ffn_gate.weight,             torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,               torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.1.post_attention_norm.weight,  torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.1.post_ffw_norm.weight,        torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_k_norm.weight,          torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,          torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_q_norm.weight,          torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,             torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.2.ffn_gate.weight,             torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,               torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.2.post_attention_norm.weight,  torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.2.post_ffw_norm.weight,        torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_k_norm.weight,          torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,          torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_q_norm.weight,          torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,             torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.3.ffn_gate.weight,             torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,               torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.3.post_attention_norm.weight,  torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.3.post_ffw_norm.weight,        torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_k_norm.weight,          torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,          torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_q_norm.weight,          torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,            torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.10.ffn_gate.weight,            torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,              torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.10.post_attention_norm.weight, torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.10.post_ffw_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_k_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_q_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,            torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.11.ffn_gate.weight,            torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,              torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.11.post_attention_norm.weight, torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.11.post_ffw_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_k_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_q_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight,            torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.12.ffn_gate.weight,            torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight,              torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.12.post_attention_norm.weight, torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.12.post_ffw_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_k_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_q_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight,            torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.13.ffn_gate.weight,            torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight,              torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.13.post_attention_norm.weight, torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.13.post_ffw_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_k_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_q_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight,            torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.14.ffn_gate.weight,            torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight,              torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.14.post_attention_norm.weight, torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.14.post_ffw_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_k_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_q_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight,            torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.15.ffn_gate.weight,            torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight,              torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.15.post_attention_norm.weight, torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.15.post_ffw_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_k_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_q_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_k_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_output.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_q_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.22.ffn_down.weight,            torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.22.ffn_gate.weight,            torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.22.ffn_up.weight,              torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.22.post_attention_norm.weight, torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.22.post_ffw_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.23.ffn_down.weight,            torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.23.ffn_gate.weight,            torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.23.ffn_up.weight,              torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.23.post_attention_norm.weight, torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.23.post_ffw_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_k_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_output.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_q_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.24.ffn_down.weight,            torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.24.ffn_gate.weight,            torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.24.ffn_up.weight,              torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.24.post_attention_norm.weight, torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.24.post_ffw_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.24.attn_k_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.24.attn_k.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.24.attn_output.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.24.attn_q_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.24.attn_q.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.24.attn_v.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.25.ffn_down.weight,            torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.25.ffn_gate.weight,            torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.25.ffn_up.weight,              torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.25.post_attention_norm.weight, torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.25.post_ffw_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.25.attn_k_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.25.attn_k.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.25.attn_output.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.25.attn_q_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.25.attn_q.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.25.attn_v.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.26.ffn_down.weight,            torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.26.ffn_gate.weight,            torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.26.ffn_up.weight,              torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.26.post_attention_norm.weight, torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.26.post_ffw_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.26.attn_k_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.26.attn_k.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.26.attn_output.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.26.attn_q_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.26.attn_q.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.26.attn_v.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.27.ffn_down.weight,            torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.27.ffn_gate.weight,            torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.27.ffn_up.weight,              torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.27.post_attention_norm.weight, torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.27.post_ffw_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.27.attn_k_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.27.attn_k.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.27.attn_output.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.27.attn_q_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.27.attn_q.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.27.attn_v.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.28.attn_k_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.28.attn_k.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.28.attn_output.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.28.attn_q_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.28.attn_q.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.28.attn_v.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.16.ffn_down.weight,            torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.16.ffn_gate.weight,            torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.16.ffn_up.weight,              torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.16.post_attention_norm.weight, torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.16.post_ffw_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.17.ffn_down.weight,            torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.17.ffn_gate.weight,            torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.17.ffn_up.weight,              torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.17.post_attention_norm.weight, torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.17.post_ffw_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_k_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_output.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_q_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.18.ffn_down.weight,            torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.18.ffn_gate.weight,            torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.18.ffn_up.weight,              torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.18.post_attention_norm.weight, torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.18.post_ffw_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_k_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_output.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_q_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.19.ffn_down.weight,            torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.19.ffn_gate.weight,            torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.19.ffn_up.weight,              torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.19.post_attention_norm.weight, torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.19.post_ffw_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_k_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_output.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_q_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.20.ffn_down.weight,            torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.20.ffn_gate.weight,            torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.20.ffn_up.weight,              torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.20.post_attention_norm.weight, torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.20.post_ffw_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.20.attn_k_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.20.attn_output.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.20.attn_q_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.21.ffn_down.weight,            torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.21.ffn_gate.weight,            torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.21.ffn_up.weight,              torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.21.post_attention_norm.weight, torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.21.post_ffw_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_k_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_output.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_q_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_k_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_output.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_q_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,             torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.4.ffn_gate.weight,             torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,               torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.4.post_attention_norm.weight,  torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.4.post_ffw_norm.weight,        torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_k_norm.weight,          torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,          torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_q_norm.weight,          torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,             torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.5.ffn_gate.weight,             torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,               torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.5.post_attention_norm.weight,  torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.5.post_ffw_norm.weight,        torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_k_norm.weight,          torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,          torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_q_norm.weight,          torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,             torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.6.ffn_gate.weight,             torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,               torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.6.post_attention_norm.weight,  torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.6.post_ffw_norm.weight,        torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_k_norm.weight,          torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,          torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_q_norm.weight,          torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,             torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.7.ffn_gate.weight,             torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,               torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.7.post_attention_norm.weight,  torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.7.post_ffw_norm.weight,        torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_k_norm.weight,          torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,          torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_q_norm.weight,          torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,             torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.8.ffn_gate.weight,             torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,               torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.8.post_attention_norm.weight,  torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.8.post_ffw_norm.weight,        torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_k_norm.weight,          torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,          torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_q_norm.weight,          torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,             torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.9.ffn_gate.weight,             torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,               torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.9.post_attention_norm.weight,  torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.9.post_ffw_norm.weight,        torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_k_norm.weight,          torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,          torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_q_norm.weight,          torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.weight,               torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:output.weight,                     torch.float32 --> Q8_0, shape = {4096, 100352}\n",
            "INFO:hf-to-gguf:blk.28.ffn_down.weight,            torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.28.ffn_gate.weight,            torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.28.ffn_up.weight,              torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.28.post_attention_norm.weight, torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.28.post_ffw_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.29.ffn_down.weight,            torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.29.ffn_gate.weight,            torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.29.ffn_up.weight,              torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.29.post_attention_norm.weight, torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.29.post_ffw_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.29.attn_k_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.29.attn_k.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.29.attn_output.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.29.attn_q_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.29.attn_q.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.29.attn_v.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.30.ffn_down.weight,            torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.30.ffn_gate.weight,            torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.30.ffn_up.weight,              torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.30.post_attention_norm.weight, torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.30.post_ffw_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.30.attn_k_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.30.attn_k.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.30.attn_output.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.30.attn_q_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.30.attn_q.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.30.attn_v.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.31.ffn_down.weight,            torch.float32 --> Q8_0, shape = {11008, 4096}\n",
            "INFO:hf-to-gguf:blk.31.ffn_gate.weight,            torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.31.ffn_up.weight,              torch.float32 --> Q8_0, shape = {4096, 11008}\n",
            "INFO:hf-to-gguf:blk.31.post_attention_norm.weight, torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.31.post_ffw_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.31.attn_k_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.31.attn_k.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.31.attn_output.weight,         torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.31.attn_q_norm.weight,         torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.31.attn_q.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.31.attn_v.weight,              torch.float32 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:output_norm.weight,                torch.float32 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:Set meta model\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:gguf: context length = 4096\n",
            "INFO:hf-to-gguf:gguf: embedding length = 4096\n",
            "INFO:hf-to-gguf:gguf: feed forward length = 11008\n",
            "INFO:hf-to-gguf:gguf: head count = 32\n",
            "INFO:hf-to-gguf:gguf: key-value head count = 32\n",
            "INFO:hf-to-gguf:gguf: rope theta = 500000\n",
            "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\n",
            "INFO:hf-to-gguf:gguf: file type = 7\n",
            "INFO:hf-to-gguf:Set model quantization version\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "INFO:numexpr.utils:NumExpr defaulting to 12 threads.\n",
            "INFO:gguf.vocab:Adding 100000 merge(s).\n",
            "INFO:gguf.vocab:Setting special token type bos to 100257\n",
            "INFO:gguf.vocab:Setting special token type eos to 100257\n",
            "INFO:gguf.vocab:Setting special token type unk to 100257\n",
            "INFO:gguf.vocab:Setting special token type pad to 100277\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:olmo2_quantized.gguf: n_tensors = 355, total_size = 7.8G\n",
            "Writing: 100% 7.76G/7.76G [01:29<00:00, 86.7Mbyte/s]\n",
            "INFO:hf-to-gguf:Model successfully exported to olmo2_quantized.gguf\n"
          ]
        }
      ],
      "source": [
        "!python3 convert_hf_to_gguf.py ../unsloth_olmo2 --outfile olmo2_quantized.gguf --outtype q8_0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "_fzqWzXPWHP_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_fzqWzXPWHP_",
        "outputId": "532cb7aa-d24c-4064-f62a-9b785d6addee"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_cbe02e65-c731-4d54-bee5-336c8fcb765a\", \"olmo2_quantized.gguf\", 7759894496)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download('olmo2_quantized.gguf')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "aac-story",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "110d3945241347eab1d81aa8f2c0d1ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cddfbe15907040cda904d7e31c1572f4",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_70492a5030664e5c8e43ecc04b3c19ef",
            "value": "â€‡6/6â€‡[00:21&lt;00:00,â€‡â€‡3.50s/it]"
          }
        },
        "3c267d2a38314955bdb4bc662aee0324": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a637fb902e4d44ea959f85508bce6ae5",
              "IPY_MODEL_c82194df3eaf489ead35feecb08b3616",
              "IPY_MODEL_110d3945241347eab1d81aa8f2c0d1ee"
            ],
            "layout": "IPY_MODEL_40495a2f13f04658bb2cf5115f35405b"
          }
        },
        "40495a2f13f04658bb2cf5115f35405b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70492a5030664e5c8e43ecc04b3c19ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a637fb902e4d44ea959f85508bce6ae5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8fb094609fb4682a946f7996003cc41",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_fca2b896ea914c70b62ac450a321c08d",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "b8fb094609fb4682a946f7996003cc41": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c82194df3eaf489ead35feecb08b3616": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce477428b3d34a3ca027dd2cfd84d1c3",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d7eb1cce81d04176873476c4e0bb3e83",
            "value": 6
          }
        },
        "cddfbe15907040cda904d7e31c1572f4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce477428b3d34a3ca027dd2cfd84d1c3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7eb1cce81d04176873476c4e0bb3e83": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fca2b896ea914c70b62ac450a321c08d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
